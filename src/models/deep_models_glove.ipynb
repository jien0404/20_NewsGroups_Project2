{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import joblib\n",
    "import gensim.downloader as api\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Setup & Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = os.path.abspath(os.path.join(os.getcwd(), \"../..\"))\n",
    "DATA_DIR = os.path.join(BASE_DIR, \"data/processed\")\n",
    "RESULTS_DIR = os.path.join(BASE_DIR, \"results/tables\")\n",
    "MODEL_DIR = os.path.join(BASE_DIR, \"models/deep_learning\")\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "train_df = pd.read_csv(os.path.join(DATA_DIR, \"train.csv\"))\n",
    "val_df   = pd.read_csv(os.path.join(DATA_DIR, \"val.csv\"))\n",
    "test_df  = pd.read_csv(os.path.join(DATA_DIR, \"test.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded labels: ['alt.atheism' 'comp.graphics' 'comp.os.ms-windows.misc'\n",
      " 'comp.sys.ibm.pc.hardware' 'comp.sys.mac.hardware' 'comp.windows.x'\n",
      " 'misc.forsale' 'rec.autos' 'rec.motorcycles' 'rec.sport.baseball'\n",
      " 'rec.sport.hockey' 'sci.crypt' 'sci.electronics' 'sci.med' 'sci.space'\n",
      " 'soc.religion.christian' 'talk.politics.guns' 'talk.politics.mideast'\n",
      " 'talk.politics.misc' 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "train_df['label_enc'] = label_encoder.fit_transform(train_df['label'])\n",
    "val_df['label_enc'] = label_encoder.transform(val_df['label'])\n",
    "test_df['label_enc'] = label_encoder.transform(test_df['label'])\n",
    "num_classes = len(label_encoder.classes_)\n",
    "joblib.dump(label_encoder, os.path.join(MODEL_DIR, \"label_encoder.pkl\"))\n",
    "print(f\"Encoded labels: {label_encoder.classes_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. GloVe Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GloVe embeddings...\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading GloVe embeddings...\")\n",
    "glove_model = api.load(\"glove-wiki-gigaword-100\")\n",
    "embedding_dim = 100\n",
    "\n",
    "def text_to_embedding_sequence(text, glove_model, embedding_dim=100):\n",
    "    tokens = text.split()\n",
    "    vectors = [glove_model[token] for token in tokens if token in glove_model]\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Padding sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting train texts to sequences...\n",
      "Converting validation texts to sequences...\n",
      "Converting test texts to sequences...\n"
     ]
    }
   ],
   "source": [
    "def pad_sequences(sequences, max_len, embedding_dim=100):\n",
    "    padded = []\n",
    "    for seq in sequences:\n",
    "        if len(seq) < max_len:\n",
    "            pad_len = max_len - len(seq)\n",
    "            seq = seq + [np.zeros(embedding_dim)] * pad_len\n",
    "        else:\n",
    "            seq = seq[:max_len]\n",
    "        padded.append(seq)\n",
    "    return np.array(padded)\n",
    "\n",
    "max_seq_len = 100\n",
    "\n",
    "print(\"Converting train texts to sequences...\")\n",
    "train_sequences = [text_to_embedding_sequence(text, glove_model, embedding_dim) for text in train_df['clean_text']]\n",
    "X_train_seq = pad_sequences(train_sequences, max_seq_len, embedding_dim)\n",
    "\n",
    "print(\"Converting validation texts to sequences...\")\n",
    "val_sequences = [text_to_embedding_sequence(text, glove_model, embedding_dim) for text in val_df['clean_text']]\n",
    "X_val_seq = pad_sequences(val_sequences, max_seq_len, embedding_dim)\n",
    "\n",
    "print(\"Converting test texts to sequences...\")\n",
    "test_sequences = [text_to_embedding_sequence(text, glove_model, embedding_dim) for text in test_df['clean_text']]\n",
    "X_test_seq = pad_sequences(test_sequences, max_seq_len, embedding_dim)\n",
    "\n",
    "y_train = train_df['label_enc'].values\n",
    "y_val = val_df['label_enc'].values\n",
    "y_test = test_df['label_enc'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Data to Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: 15997 samples\n",
      "Validation set: 2000 samples\n",
      "Test set: 2000 samples\n"
     ]
    }
   ],
   "source": [
    "X_train_tensor = torch.tensor(X_train_seq, dtype=torch.float32)\n",
    "X_val_tensor   = torch.tensor(X_val_seq, dtype=torch.float32)\n",
    "X_test_tensor  = torch.tensor(X_test_seq, dtype=torch.float32)\n",
    "\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "y_val_tensor   = torch.tensor(y_val, dtype=torch.long)\n",
    "y_test_tensor  = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset   = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "test_dataset  = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "print(f\"Train set: {len(train_dataset)} samples\")\n",
    "print(f\"Validation set: {len(val_dataset)} samples\")\n",
    "print(f\"Test set: {len(test_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Model Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim=128, num_layers=1, output_dim=2, dropout_rate=0.5):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_dim, \n",
    "            hidden_size=hidden_dim, \n",
    "            num_layers=num_layers, \n",
    "            batch_first=True, \n",
    "            dropout=dropout_rate if num_layers > 1 else 0\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, (hn, cn) = self.lstm(x)\n",
    "        hidden = hn[-1]\n",
    "        hidden = self.dropout(hidden)\n",
    "        out = self.fc(hidden)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim, max_seq_len, num_classes, num_filters=100, kernel_sizes=[3, 4, 5], dropout_rate=0.5):\n",
    "        super(CNNClassifier, self).__init__()\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv1d(in_channels=embedding_dim, out_channels=num_filters, kernel_size=k) for k in kernel_sizes\n",
    "        ])\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc = nn.Linear(num_filters * len(kernel_sizes), num_classes)\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)  # (batch, seq_len, embedding_dim) -> (batch, embedding_dim, seq_len)\n",
    "        x = [F.relu(conv(x)).max(dim=2)[0] for conv in self.convs]  # Convolution + Global Max Pooling\n",
    "        x = torch.cat(x, dim=1)  # Concatenate feature maps\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_heads, num_layers, output_dim, dropout_rate=0.5):\n",
    "        super(TransformerClassifier, self).__init__()\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=num_heads, dropout=dropout_rate),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc = nn.Linear(embedding_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, seq_len, embedding_dim]\n",
    "        x = self.transformer_encoder(x.permute(1, 0, 2)) # [seq_len, batch_size, embedding_dim]\n",
    "        # average the embeddings across the sequence length\n",
    "        x = torch.mean(x, dim=0) # [batch_size, embedding_dim]\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x) # [batch_size, output_dim]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs=20, patience=5, model_name=\"Model\"):\n",
    "    model = model.to(device) #Move model to device\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    best_val_acc = 0\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct, total = 0, 0\n",
    "\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} - {model_name}\", leave=False)\n",
    "\n",
    "        for X_batch, y_batch in progress_bar:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device) #Move data to device\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += y_batch.size(0)\n",
    "            correct += (predicted == y_batch).sum().item()\n",
    "\n",
    "            progress_bar.set_postfix(loss=loss.item(), acc=100 * correct / total)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_correct, val_total = 0, 0\n",
    "        val_predictions = []\n",
    "        val_true_labels = []\n",
    "        total_val_loss = 0.0  # Initialize total_val_loss\n",
    "        val_report = {} #initialise to empty dictionary\n",
    "        with torch.no_grad():\n",
    "            for X_val_batch, y_val_batch in val_loader:\n",
    "                X_val_batch, y_val_batch = X_val_batch.to(device), y_val_batch.to(device) #Move data to device\n",
    "                val_outputs = model(X_val_batch)\n",
    "                loss = criterion(val_outputs, y_val_batch)\n",
    "                total_val_loss += loss.item()\n",
    "                _, val_predicted = torch.max(val_outputs, 1)\n",
    "                val_total += y_val_batch.size(0)\n",
    "                val_correct += (val_predicted == y_val_batch).sum().item()\n",
    "                val_predictions.extend(val_predicted.cpu().numpy())\n",
    "                val_true_labels.extend(y_val_batch.cpu().numpy())\n",
    "\n",
    "        val_acc = 100 * val_correct / val_total\n",
    "        avg_val_loss = total_val_loss / len(val_loader) if len(val_loader) > 0 else 0.0 #handle case of val_loader being empty\n",
    "        val_report = classification_report(val_true_labels, val_predictions, output_dict=True, zero_division=0) if len(val_loader) > 0 else {}\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Loss: {running_loss/len(train_loader):.4f} - Val Loss: {avg_val_loss:.4f} - Val Acc: {val_acc:.2f}% - {model_name}\")\n",
    "\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            best_val_acc = val_acc\n",
    "            epochs_no_improve = 0\n",
    "            best_model_state = model.state_dict()\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"Early stopping triggered at epoch {epoch+1} - {model_name}\")\n",
    "            break\n",
    "\n",
    "    model.load_state_dict(best_model_state)\n",
    "    print(f\"Best Validation Accuracy: {best_val_acc:.2f}% - {model_name}\")\n",
    "    return model, best_val_acc/100, val_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, model_name=\"Model\"):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device) #Move data to device\n",
    "            outputs = model(X_batch)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            predictions.extend(predicted.cpu().numpy())\n",
    "            true_labels.extend(y_batch.cpu().numpy())\n",
    "\n",
    "    test_accuracy = accuracy_score(true_labels, predictions)\n",
    "    report = classification_report(true_labels, predictions, output_dict=True, zero_division=0)\n",
    "    print(f\"Test Accuracy: {test_accuracy:.2f}% - {model_name}\")\n",
    "    print(classification_report(true_labels, predictions))\n",
    "    return test_accuracy, report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(model_name, test_accuracy, report, val_accuracy, val_report):\n",
    "    results = {\n",
    "        \"Model\": model_name,\n",
    "        \"Test_Accuracy\": test_accuracy,\n",
    "        \"Test_Precision\": report['weighted avg']['precision'],\n",
    "        \"Test_Recall\": report['weighted avg']['recall'],\n",
    "        \"Test_F1-Score\": report['weighted avg']['f1-score'],\n",
    "        \"Val_Accuracy\": val_accuracy,\n",
    "        \"Val_Precision\": val_report['weighted avg']['precision'],\n",
    "        \"Val_Recall\": val_report['weighted avg']['recall'],\n",
    "        \"Val_F1-Score\": val_report['weighted avg']['f1-score']\n",
    "    }\n",
    "    results_df = pd.DataFrame([results])\n",
    "    results_file = os.path.join(RESULTS_DIR, \"deep_models_results.csv\")\n",
    "    if not os.path.exists(results_file):\n",
    "        results_df.to_csv(results_file, index=False, header=True)\n",
    "    else:\n",
    "        results_df.to_csv(results_file, index=False, header=False, mode='a')\n",
    "    print(f\"Deep Learning results saved to: {results_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 - LSTM:   0%|          | 0/500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 - Loss: 2.3894 - Val Loss: 2.1036 - Val Acc: 28.35% - LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20 - Loss: 2.1178 - Val Loss: 1.8180 - Val Acc: 38.90% - LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20 - Loss: 1.6990 - Val Loss: 1.2972 - Val Acc: 51.70% - LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20 - Loss: 1.5943 - Val Loss: 1.5545 - Val Acc: 45.85% - LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20 - Loss: 1.1385 - Val Loss: 0.8818 - Val Acc: 67.50% - LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20 - Loss: 0.7065 - Val Loss: 0.5849 - Val Acc: 73.45% - LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20 - Loss: 0.6503 - Val Loss: 0.5057 - Val Acc: 80.20% - LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20 - Loss: 0.5354 - Val Loss: 0.3622 - Val Acc: 84.80% - LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20 - Loss: 0.3213 - Val Loss: 0.2248 - Val Acc: 90.85% - LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20 - Loss: 0.3199 - Val Loss: 0.1950 - Val Acc: 91.65% - LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20 - Loss: 1.7373 - Val Loss: 1.0801 - Val Acc: 67.80% - LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20 - Loss: 0.6226 - Val Loss: 0.3304 - Val Acc: 86.00% - LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20 - Loss: 0.3700 - Val Loss: 0.2232 - Val Acc: 89.95% - LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20 - Loss: 0.2663 - Val Loss: 0.1689 - Val Acc: 94.45% - LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20 - Loss: 0.1973 - Val Loss: 0.3564 - Val Acc: 89.15% - LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20 - Loss: 0.1676 - Val Loss: 0.0934 - Val Acc: 96.05% - LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20 - Loss: 0.1311 - Val Loss: 0.0914 - Val Acc: 96.95% - LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20 - Loss: 0.1462 - Val Loss: 0.0785 - Val Acc: 96.70% - LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20 - Loss: 0.1697 - Val Loss: 0.1809 - Val Acc: 93.90% - LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20 - Loss: 0.1396 - Val Loss: 0.0848 - Val Acc: 96.20% - LSTM\n",
      "Best Validation Accuracy: 96.70% - LSTM\n",
      "Test Accuracy: 0.96% - LSTM\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.84      0.85       100\n",
      "           1       0.99      0.99      0.99       100\n",
      "           2       1.00      0.96      0.98       100\n",
      "           3       0.98      1.00      0.99       100\n",
      "           4       1.00      1.00      1.00       100\n",
      "           5       1.00      1.00      1.00       100\n",
      "           6       0.99      0.98      0.98       100\n",
      "           7       1.00      0.99      0.99       100\n",
      "           8       0.99      1.00      1.00       100\n",
      "           9       1.00      1.00      1.00       100\n",
      "          10       0.99      1.00      1.00       100\n",
      "          11       0.98      1.00      0.99       100\n",
      "          12       0.98      1.00      0.99       100\n",
      "          13       0.99      1.00      1.00       100\n",
      "          14       1.00      0.99      0.99       100\n",
      "          15       1.00      1.00      1.00       100\n",
      "          16       0.92      0.99      0.95       100\n",
      "          17       0.96      0.99      0.98       100\n",
      "          18       0.88      0.82      0.85       100\n",
      "          19       0.76      0.74      0.75       100\n",
      "\n",
      "    accuracy                           0.96      2000\n",
      "   macro avg       0.96      0.96      0.96      2000\n",
      "weighted avg       0.96      0.96      0.96      2000\n",
      "\n",
      "Deep Learning results saved to: d:\\E\\2024.2\\project_2\\Project2_20-NewsGroups\\results/tables\\deep_models_results.csv\n"
     ]
    }
   ],
   "source": [
    "lstm_model = LSTMClassifier(embedding_dim, hidden_dim=128, num_layers=1, output_dim=num_classes, dropout_rate=0.5)\n",
    "lstm_model, val_accuracy_lstm, val_report_lstm = train_model(lstm_model, train_loader, val_loader, criterion, optim.Adam(lstm_model.parameters(), lr=0.001), epochs=20, model_name=\"LSTM\")\n",
    "test_accuracy_lstm, report_lstm = evaluate_model(lstm_model, test_loader, model_name=\"LSTM\")\n",
    "save_results(\"LSTMClassifier\", test_accuracy_lstm, report_lstm, val_accuracy_lstm, val_report_lstm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 - Loss: 0.4117 - Val Loss: 0.0873 - Val Acc: 97.00% - CNN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20 - Loss: 0.1507 - Val Loss: 0.1065 - Val Acc: 96.85% - CNN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20 - Loss: 0.1446 - Val Loss: 0.0888 - Val Acc: 96.90% - CNN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20 - Loss: 0.1534 - Val Loss: 0.1031 - Val Acc: 96.55% - CNN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20 - Loss: 0.1563 - Val Loss: 0.0922 - Val Acc: 96.85% - CNN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20 - Loss: 0.1541 - Val Loss: 0.0899 - Val Acc: 96.70% - CNN\n",
      "Early stopping triggered at epoch 6 - CNN\n",
      "Best Validation Accuracy: 97.00% - CNN\n",
      "Test Accuracy: 0.97% - CNN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.84      0.88       100\n",
      "           1       0.99      0.99      0.99       100\n",
      "           2       1.00      1.00      1.00       100\n",
      "           3       1.00      1.00      1.00       100\n",
      "           4       1.00      1.00      1.00       100\n",
      "           5       1.00      0.99      0.99       100\n",
      "           6       1.00      1.00      1.00       100\n",
      "           7       1.00      0.98      0.99       100\n",
      "           8       1.00      1.00      1.00       100\n",
      "           9       1.00      1.00      1.00       100\n",
      "          10       1.00      1.00      1.00       100\n",
      "          11       0.99      1.00      1.00       100\n",
      "          12       0.97      1.00      0.99       100\n",
      "          13       1.00      1.00      1.00       100\n",
      "          14       1.00      1.00      1.00       100\n",
      "          15       1.00      1.00      1.00       100\n",
      "          16       0.97      0.92      0.94       100\n",
      "          17       0.97      0.97      0.97       100\n",
      "          18       0.90      0.79      0.84       100\n",
      "          19       0.71      0.90      0.80       100\n",
      "\n",
      "    accuracy                           0.97      2000\n",
      "   macro avg       0.97      0.97      0.97      2000\n",
      "weighted avg       0.97      0.97      0.97      2000\n",
      "\n",
      "Deep Learning results saved to: d:\\E\\2024.2\\project_2\\Project2_20-NewsGroups\\results/tables\\deep_models_results.csv\n"
     ]
    }
   ],
   "source": [
    "cnn_model = CNNClassifier(embedding_dim, max_seq_len, num_classes)\n",
    "cnn_model, val_accuracy_cnn, val_report_cnn = train_model(cnn_model, train_loader, val_loader, criterion, optim.Adam(cnn_model.parameters(), lr=0.003), epochs=20, model_name=\"CNN\")\n",
    "test_accuracy_cnn, report_cnn = evaluate_model(cnn_model, test_loader, model_name=\"CNN\")\n",
    "save_results(\"CNNClassifier\", test_accuracy_cnn, report_cnn, val_accuracy_cnn, val_report_cnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DO XUAN CHIEN\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n",
      "                                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 - Loss: 1.2015 - Val Loss: 0.3059 - Val Acc: 84.85% - Transformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20 - Loss: 0.3215 - Val Loss: 0.2842 - Val Acc: 87.30% - Transformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20 - Loss: 0.2389 - Val Loss: 0.1299 - Val Acc: 95.20% - Transformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20 - Loss: 0.1506 - Val Loss: 0.1746 - Val Acc: 93.05% - Transformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20 - Loss: 0.1289 - Val Loss: 0.0842 - Val Acc: 96.30% - Transformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20 - Loss: 0.1146 - Val Loss: 0.1088 - Val Acc: 95.50% - Transformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20 - Loss: 0.1095 - Val Loss: 0.1632 - Val Acc: 95.50% - Transformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20 - Loss: 0.1054 - Val Loss: 0.1490 - Val Acc: 95.25% - Transformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20 - Loss: 0.0965 - Val Loss: 0.1255 - Val Acc: 96.10% - Transformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20 - Loss: 0.0923 - Val Loss: 0.1108 - Val Acc: 96.35% - Transformer\n",
      "Early stopping triggered at epoch 10 - Transformer\n",
      "Best Validation Accuracy: 96.30% - Transformer\n",
      "Test Accuracy: 0.97% - Transformer\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.83      0.90       100\n",
      "           1       0.99      0.96      0.97       100\n",
      "           2       0.98      0.99      0.99       100\n",
      "           3       1.00      0.99      0.99       100\n",
      "           4       0.98      1.00      0.99       100\n",
      "           5       0.98      0.98      0.98       100\n",
      "           6       1.00      0.99      0.99       100\n",
      "           7       0.99      1.00      1.00       100\n",
      "           8       1.00      1.00      1.00       100\n",
      "           9       1.00      0.99      0.99       100\n",
      "          10       0.99      1.00      1.00       100\n",
      "          11       0.99      1.00      1.00       100\n",
      "          12       1.00      1.00      1.00       100\n",
      "          13       0.99      1.00      1.00       100\n",
      "          14       0.99      1.00      1.00       100\n",
      "          15       1.00      1.00      1.00       100\n",
      "          16       0.92      0.94      0.93       100\n",
      "          17       0.97      0.98      0.98       100\n",
      "          18       0.99      0.73      0.84       100\n",
      "          19       0.72      0.98      0.83       100\n",
      "\n",
      "    accuracy                           0.97      2000\n",
      "   macro avg       0.97      0.97      0.97      2000\n",
      "weighted avg       0.97      0.97      0.97      2000\n",
      "\n",
      "Deep Learning results saved to: d:\\E\\2024.2\\project_2\\Project2_20-NewsGroups\\results/tables\\deep_models_results.csv\n"
     ]
    }
   ],
   "source": [
    "transformer_model = TransformerClassifier(embedding_dim=embedding_dim, num_heads=10, num_layers=2, output_dim=num_classes, dropout_rate=0.5)\n",
    "transformer_model, val_accuracy_transformer, val_report_transformer = train_model(transformer_model, train_loader, val_loader, criterion, optim.Adam(transformer_model.parameters(), lr=0.001), epochs=20, model_name=\"Transformer\")\n",
    "test_accuracy_transformer, report_transformer = evaluate_model(transformer_model, test_loader, model_name=\"Transformer\")\n",
    "save_results(\"TransformerClassifier\", test_accuracy_transformer, report_transformer, val_accuracy_transformer, val_report_transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Save Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(lstm_model.state_dict(), os.path.join(MODEL_DIR, \"lstm_model.pth\"))\n",
    "torch.save(cnn_model.state_dict(), os.path.join(MODEL_DIR, \"cnn_model.pth\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
